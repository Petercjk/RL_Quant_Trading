# RL Quantitative Trading Agent (A-Share)
# åŸºäºå¼ºåŒ–å­¦ä¹ çš„é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“

> ğŸš§ Work in Progress / æ–½å·¥ä¸­**
>
> This repository is currently under active development for my undergraduate thesis.
> æœ¬é¡¹ç›®ä¸ºæˆ‘çš„æœ¬ç§‘æ¯•ä¸šè®ºæ–‡ä»£ç åº“ï¼Œç›®å‰æ­£åœ¨æŒç»­å¼€å‘å’Œæ›´æ–°ä¸­ã€‚

---

## Introduction / é¡¹ç›®ç®€ä»‹

Welcome! This is the repository for my undergraduate thesis: "Design and Implementation of a Reinforcement Learning-based Quantitative Trading Agent".

In simple terms, I'm training AI agents (using RL) to find profitable trading strategies within the complex environment of the Chinese A-share market. Instead of just predicting prices, the agent is learning how to make decisions (buy/sell/hold) to optimize portfolio value over time.

æ¬¢è¿ï¼è¿™æ˜¯æˆ‘çš„æœ¬ç§‘æ¯•ä¸šè®ºæ–‡ã€ŠåŸºäºå¼ºåŒ–å­¦ä¹ çš„é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“è®¾è®¡ä¸å®ç°ã€‹çš„ä»£ç ä»“åº“ã€‚

ç®€å•æ¥è¯´ï¼Œæˆ‘æ­£åœ¨å°è¯•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œè®­ç»ƒä¸€ä¸ªèƒ½åœ¨ä¸­å›½Aè‚¡å¸‚åœºé‡Œåˆ¶å®šç­–ç•¥çš„AIæ™ºèƒ½ä½“ã€‚ä¸ä¼ ç»Ÿè‚¡ä»·é¢„æµ‹ä¸åŒï¼Œè¿™ä¸ªé¡¹ç›®çš„æ ¸å¿ƒåœ¨äºè®­ç»ƒæ™ºèƒ½ä½“å­¦ä¼šåœ¨åŠ¨æ€çš„å¸‚åœºç¯å¢ƒä¸­åšå†³ç­–ï¼ˆä¹°å…¥ã€å–å‡ºæˆ–æŒä»“ï¼‰ï¼Œä»è€Œå®ç°èµ„äº§å¢å€¼ã€‚

## Key Features / æ ¸å¿ƒå†…å®¹

* Market: Chinese A-Share market (T+1 trading rule, price limits, etc).
    * å¸‚åœºï¼š èšç„¦ä¸­å›½ A è‚¡ï¼ˆè€ƒè™‘ T+1ã€æ¶¨è·Œåœé™åˆ¶ç­‰ç‰¹æœ‰è§„åˆ™ï¼‰ã€‚
* Method: Deep Reinforcement Learning (PPO algorithm focus).
    * æ–¹æ³•ï¼š æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆPPO ç®—æ³•ç­‰ï¼‰ã€‚
* Evaluation: Focusing on Online Trading simulation, not just backtesting on training data.
    * è¯„ä¼°ï¼š é‡ç‚¹åœ¨äºæ¨¡æ‹Ÿå®ç›˜å†³ç­–ï¼Œä¸ä»…ä»…æ˜¯å†å²æ•°æ®çš„ç®€å•å›æµ‹ã€‚
* Data Source: Tushare API (Data is not included in this repo).
    * æ•°æ®æºï¼šTushare APIï¼ˆåŸå§‹æ•°æ®æ–‡ä»¶ä¸åŒ…å«åœ¨ä»“åº“ä¸­ï¼‰ã€‚

## Project Structure / é¡¹ç›®ç»“æ„

Here is how the project is organized. This structure is designed to separate configuration, core logic, and experimental results.
è¿™é‡Œæ˜¯é¡¹ç›®ç»“æ„å®‰æ’ã€‚é¡¹ç›®é‡‡ç”¨äº†æ¨¡å—åŒ–ç»“æ„è®¾è®¡ï¼Œå°†é…ç½®ã€æ ¸å¿ƒé€»è¾‘ä¸å®éªŒç»“æœåˆ†ç¦»ï¼Œä»¥ä¾¿äºå¤ç°å’Œæ‰©å±•ã€‚

```text
RL_Quant_Trading/
â”‚
â”œâ”€â”€ configs/                   # é…ç½®æ–‡ä»¶ (YAML)
â”‚   â”œâ”€â”€ agent/                 # æ™ºèƒ½ä½“å‚æ•°é…ç½® (PPO, A2C ç­‰)
â”‚   â”‚   â””â”€â”€ __pycache__/      
â”‚   â”œâ”€â”€ env/                   # ç¯å¢ƒé…ç½® (äº¤æ˜“æˆæœ¬ã€çª—å£å¤§å°ç­‰)
â”‚   â”‚   â””â”€â”€ __pycache__/      
â”‚   â”œâ”€â”€ experiment/            # å®éªŒé…ç½®(æ¯æ¬¡è®­ç»ƒä¸ºå•ç‹¬ä¸€æ¬¡å®éªŒ)
â”‚   â”‚   â””â”€â”€ __pycache__/      
â”‚   â””â”€â”€ __pycache__/           #
â”‚
â”œâ”€â”€ data/                      # æ•°æ®å­˜å‚¨ (å·²åœ¨ .gitignore ä¸­å¿½ç•¥)
â”‚   â”œâ”€â”€ raw/                   # åŸå§‹æ•°æ® (Tushare ä¸‹è½½)
â”‚   â””â”€â”€ processed/             # æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹åçš„æ•°æ®
â”‚
â”œâ”€â”€ docs/                      # æ–‡æ¡£ä¸ç¬”è®°
â”‚   â””â”€â”€ experiments/           # å®éªŒç»“æœæ•°æ® (å·²åœ¨ .gitignore ä¸­å¿½ç•¥)
â”‚       â””â”€â”€ 20260129_0014_base_experiment
â”‚           â”œâ”€â”€ checkpoints   # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚           â”œâ”€â”€ logs          # æ—¥å¿—
â”‚           â”‚   â””â”€â”€ PPO_1     # PPO å•ç‹¬è®­ç»ƒæ—¥å¿—
â”‚           â”œâ”€â”€ plots         # ç»˜å›¾
â”‚           â””â”€â”€ tables        # è¡¨æ ¼æ•°æ®
â”‚
â”œâ”€â”€ hyperparam_search/         # è¶…å‚æ•°æœç´¢è„šæœ¬å’Œç»“æœï¼ˆæœªå¼€å§‹ï¼‰
â”‚
â”œâ”€â”€ online/                    # åœ¨çº¿/æ»šåŠ¨é¢„æµ‹ï¼ˆæœªå¼€å§‹ï¼‰
â”‚   â”œâ”€â”€ online_trader.py       # æ¨¡æ‹Ÿå®æ—¶äº¤æ˜“å†³ç­–
â”‚   â””â”€â”€ rolling_test.py        # æ»šåŠ¨å›æµ‹åˆ†æ
â”‚
â””â”€â”€ src/                       # æ ¸å¿ƒæºä»£ç 
    â”œâ”€â”€ data_processing/       # æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹è„šæœ¬
    â”‚   â””â”€â”€ __pycache__/      
    â”œâ”€â”€ envs/                  # è‡ªå®šä¹‰äº¤æ˜“ç¯å¢ƒï¼ˆå…¼å®¹Gymï¼Œå‚è€ƒFinRLå®šä¹‰ï¼‰
    â”‚   â””â”€â”€ __pycache__/      
    â”œâ”€â”€ models/                # ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆæœªå¼€å¯ï¼‰
    â”œâ”€â”€ training/              
    â”‚   â””â”€â”€ __pycache__/      
    â””â”€â”€ utils/                 # å·¥å…·å‡½æ•° (æ—¥å¿—è®°å½•ã€éšæœºç§å­ç­‰)
